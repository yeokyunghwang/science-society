{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06df127c-ebc2-4bd8-b4f8-d4d8cb897db3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dec34b1a-33a7-41e2-9816-9d68176690dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.26.4\n",
      "  Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: pandas==2.2.2 in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas==2.2.2) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.2.2) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas==2.2.2) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas==2.2.2) (1.16.0)\n",
      "Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.24.4\n",
      "    Uninstalling numpy-1.24.4:\n",
      "      Successfully uninstalled numpy-1.24.4\n",
      "Successfully installed numpy-1.26.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numpy 1.26.4\n",
      "pandas 2.2.2\n",
      "Collecting datasets\n",
      "  Using cached datasets-4.5.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting tokenizers\n",
      "  Using cached tokenizers-0.22.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Using cached pyarrow-23.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.27.0)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.6.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.19 (from datasets)\n",
      "  Using cached multiprocess-0.70.18-py310-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2024.2.0)\n",
      "Collecting huggingface-hub<2.0,>=0.25.0 (from datasets)\n",
      "  Using cached huggingface_hub-1.4.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Using cached aiohttp-3.13.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1.0.0->datasets) (3.7.1)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1.0.0->datasets) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1.0.0->datasets) (1.0.5)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx<1.0.0->datasets) (3.7)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.14.0)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface-hub<2.0,>=0.25.0->datasets)\n",
      "  Using cached hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting shellingham (from huggingface-hub<2.0,>=0.25.0->datasets)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting typer-slim (from huggingface-hub<2.0,>=0.25.0->datasets)\n",
      "  Using cached typer_slim-0.21.1-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.12.2)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Using cached aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting async-timeout<6.0,>=4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Using cached async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (24.2.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Using cached frozenlist-1.8.0-cp310-cp310-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (20 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Using cached multidict-6.7.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Using cached propcache-0.4.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Using cached yarl-1.22.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1.0.0->datasets) (1.2.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (8.1.7)\n",
      "Using cached datasets-4.5.0-py3-none-any.whl (515 kB)\n",
      "Using cached dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Using cached huggingface_hub-1.4.1-py3-none-any.whl (553 kB)\n",
      "Using cached hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "Using cached multiprocess-0.70.18-py310-none-any.whl (134 kB)\n",
      "Using cached tokenizers-0.22.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "Using cached aiohttp-3.13.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\n",
      "Using cached async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\n",
      "Using cached multidict-6.7.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (243 kB)\n",
      "Using cached yarl-1.22.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (346 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Using cached frozenlist-1.8.0-cp310-cp310-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (219 kB)\n",
      "Using cached propcache-0.4.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (196 kB)\n",
      "Using cached pyarrow-23.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (47.5 MB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached typer_slim-0.21.1-py3-none-any.whl (47 kB)\n",
      "Using cached xxhash-3.6.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
      "Installing collected packages: xxhash, typer-slim, shellingham, pyarrow, propcache, multidict, hf-xet, frozenlist, dill, async-timeout, aiohappyeyeballs, yarl, multiprocess, aiosignal, huggingface-hub, aiohttp, tokenizers, datasets\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18/18\u001b[0m [datasets]/18\u001b[0m [datasets]ce-hub]\n",
      "\u001b[1A\u001b[2KSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.13.3 aiosignal-1.4.0 async-timeout-5.0.1 datasets-4.5.0 dill-0.4.0 frozenlist-1.8.0 hf-xet-1.2.0 huggingface-hub-1.4.1 multidict-6.7.1 multiprocess-0.70.18 propcache-0.4.1 pyarrow-23.0.0 shellingham-1.5.4 tokenizers-0.22.2 typer-slim-0.21.1 xxhash-3.6.0 yarl-1.22.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping transformers as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Skipping accelerate as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.43.4\n",
      "  Using cached transformers-4.43.4-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting accelerate==0.33.0\n",
      "  Using cached accelerate-0.33.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.43.4) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers==4.43.4)\n",
      "  Using cached huggingface_hub-0.36.2-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.43.4) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.43.4) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.43.4) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers==4.43.4)\n",
      "  Using cached regex-2026.1.15-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.43.4) (2.32.3)\n",
      "Collecting safetensors>=0.4.1 (from transformers==4.43.4)\n",
      "  Using cached safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers==4.43.4)\n",
      "  Using cached tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.43.4) (4.66.5)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.33.0) (5.9.8)\n",
      "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.33.0) (2.3.1+cu121)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.43.4) (2024.2.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.43.4) (1.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers==4.43.4) (4.12.2)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.33.0) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.33.0) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.33.0) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.33.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.33.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.33.0) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.33.0) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.33.0) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.33.0) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.33.0) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.33.0) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.33.0) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.33.0) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.33.0) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.33.0) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate==0.33.0) (12.1.105)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.33.0) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.43.4) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.43.4) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.43.4) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.43.4) (2024.7.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.33.0) (1.3.0)\n",
      "Using cached transformers-4.43.4-py3-none-any.whl (9.4 MB)\n",
      "Using cached accelerate-0.33.0-py3-none-any.whl (315 kB)\n",
      "Using cached huggingface_hub-0.36.2-py3-none-any.whl (566 kB)\n",
      "Using cached tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Using cached regex-2026.1.15-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (791 kB)\n",
      "Using cached safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\n",
      "Installing collected packages: safetensors, regex, huggingface-hub, tokenizers, transformers, accelerate\n",
      "\u001b[2K  Attempting uninstall: huggingface-hub\n",
      "\u001b[2K    Found existing installation: huggingface_hub 1.4.1\n",
      "\u001b[2K    Uninstalling huggingface_hub-1.4.1:\n",
      "\u001b[2K      Successfully uninstalled huggingface_hub-1.4.1\n",
      "\u001b[2K  Attempting uninstall: tokenizersm\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [huggingface-hub]\n",
      "\u001b[2K    Found existing installation: tokenizers 0.22.2━━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [huggingface-hub]\n",
      "\u001b[2K    Uninstalling tokenizers-0.22.2:m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [huggingface-hub]\n",
      "\u001b[2K      Successfully uninstalled tokenizers-0.22.2━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/6\u001b[0m [huggingface-hub]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6/6\u001b[0m [accelerate]6\u001b[0m [accelerate]s]\n",
      "\u001b[1A\u001b[2KSuccessfully installed accelerate-0.33.0 huggingface-hub-0.36.2 regex-2026.1.15 safetensors-0.7.0 tokenizers-0.19.1 transformers-4.43.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install -U \"numpy==1.26.4\" \"pandas==2.2.2\"\n",
    "python - <<'PY'\n",
    "import numpy as np, pandas as pd\n",
    "print(\"numpy\", np.__version__)\n",
    "print(\"pandas\", pd.__version__)\n",
    "PY\n",
    "pip install datasets tokenizers \n",
    "pip uninstall -y transformers accelerate\n",
    "pip install -U \"transformers==4.43.4\" \"accelerate==0.33.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce2f7c36-1ddb-4cb9-9af4-814743ab752f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\"`tokenizer` is deprecated and will be removed in version 5.0.0\"\n",
    ")\n",
    "\n",
    "import transformers\n",
    "transformers.utils.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cc8a37-8426-4b22-b74b-0fa4aa6b9e45",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## make walk & data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6537b2ec-c460-4ce3-b43f-ce9383a9a5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] start_mode=per_node scheme=degree wrote 20000 walks to ./corpus/start=per_node__scheme=degree/news_1990.txt\n",
      "[OK] start_mode=per_node scheme=degree wrote 20000 walks to ./corpus/start=per_node__scheme=degree/paper_1990.txt\n",
      "[OK] start_mode=per_node scheme=degree wrote 20000 walks to ./corpus/start=per_node__scheme=degree/news_1991.txt\n",
      "[OK] start_mode=per_node scheme=degree wrote 20000 walks to ./corpus/start=per_node__scheme=degree/paper_1991.txt\n",
      "[OK] start_mode=per_node scheme=degree wrote 20000 walks to ./corpus/start=per_node__scheme=degree/news_1992.txt\n",
      "[OK] start_mode=per_node scheme=degree wrote 20000 walks to ./corpus/start=per_node__scheme=degree/paper_1992.txt\n",
      "[OK] start_mode=per_node scheme=degree wrote 20000 walks to ./corpus/start=per_node__scheme=degree/news_1993.txt\n",
      "[OK] start_mode=per_node scheme=degree wrote 20000 walks to ./corpus/start=per_node__scheme=degree/paper_1993.txt\n",
      "[OK] start_mode=per_node scheme=degree wrote 20000 walks to ./corpus/start=per_node__scheme=degree/news_1994.txt\n",
      "[OK] start_mode=per_node scheme=degree wrote 20000 walks to ./corpus/start=per_node__scheme=degree/paper_1994.txt\n",
      "[OK] start_mode=per_node scheme=degree wrote 20000 walks to ./corpus/start=per_node__scheme=degree/news_1995.txt\n",
      "[OK] start_mode=per_node scheme=degree wrote 20000 walks to ./corpus/start=per_node__scheme=degree/paper_1995.txt\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd /root/science-society\n",
    "\n",
    "START_MODE=\"per_node\"\n",
    "PER_NODE_SCHEME=\"degree\"\n",
    "\n",
    "if [ \"${START_MODE}\" = \"per_node\" ]; then\n",
    "  MODE_TAG=\"start=${START_MODE}__scheme=${PER_NODE_SCHEME}\"\n",
    "else\n",
    "  MODE_TAG=\"start=${START_MODE}\"\n",
    "fi\n",
    "\n",
    "CORPUS_DIR=\"./corpus/${MODE_TAG}\"\n",
    "MAP_DIR=\"./tokenizer/${MODE_TAG}\"\n",
    "mkdir -p \"${CORPUS_DIR}\" \"${MAP_DIR}\"\n",
    "\n",
    "for Y in 1990 1991 1992 1993 1994 1995; do\n",
    "  python ./scripts/make_walk_corpus_encoded.py \\\n",
    "    --pkl ./data/news/news_network_${Y}_m0.0-M1.0.pkl \\\n",
    "    --out \"${CORPUS_DIR}/news_${Y}.txt\" \\\n",
    "    --walk_length 20 --num_walks 20000 --seed 42 \\\n",
    "    --start_mode \"${START_MODE}\" --per_node_scheme \"${PER_NODE_SCHEME}\" \\\n",
    "    --mapping_json \"${MAP_DIR}/news_mapping_${Y}.json\"\n",
    "\n",
    "  python ./scripts/make_walk_corpus_encoded.py \\\n",
    "    --pkl ./data/paper/paper_network_${Y}_m0.0-M0.5.pkl \\\n",
    "    --out \"${CORPUS_DIR}/paper_${Y}.txt\" \\\n",
    "    --walk_length 20 --num_walks 20000 --seed 42 \\\n",
    "    --start_mode \"${START_MODE}\" --per_node_scheme \"${PER_NODE_SCHEME}\" \\\n",
    "    --mapping_json \"${MAP_DIR}/paper_mapping_${Y}.json\"\n",
    "done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bb09fdaf-8ac6-4649-9198-a192689ec451",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 20000/20000 [00:00<00:00, 1598347.66 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] saved news_1990 -> ./datasets/start=per_node__scheme=degree/news_1990 (n=20000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 20000/20000 [00:00<00:00, 1586587.99 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] saved paper_1990 -> ./datasets/start=per_node__scheme=degree/paper_1990 (n=20000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 20000/20000 [00:00<00:00, 1649255.45 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] saved news_1991 -> ./datasets/start=per_node__scheme=degree/news_1991 (n=20000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 20000/20000 [00:00<00:00, 1588721.43 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] saved paper_1991 -> ./datasets/start=per_node__scheme=degree/paper_1991 (n=20000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 20000/20000 [00:00<00:00, 1641895.44 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] saved news_1992 -> ./datasets/start=per_node__scheme=degree/news_1992 (n=20000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 20000/20000 [00:00<00:00, 1510834.79 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] saved paper_1992 -> ./datasets/start=per_node__scheme=degree/paper_1992 (n=20000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 20000/20000 [00:00<00:00, 1664010.16 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] saved news_1993 -> ./datasets/start=per_node__scheme=degree/news_1993 (n=20000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 20000/20000 [00:00<00:00, 1521798.16 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] saved paper_1993 -> ./datasets/start=per_node__scheme=degree/paper_1993 (n=20000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 20000/20000 [00:00<00:00, 1637057.10 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] saved news_1994 -> ./datasets/start=per_node__scheme=degree/news_1994 (n=20000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 20000/20000 [00:00<00:00, 1480987.25 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] saved paper_1994 -> ./datasets/start=per_node__scheme=degree/paper_1994 (n=20000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 20000/20000 [00:00<00:00, 1531830.10 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] saved news_1995 -> ./datasets/start=per_node__scheme=degree/news_1995 (n=20000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 20000/20000 [00:00<00:00, 1487658.37 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] saved paper_1995 -> ./datasets/start=per_node__scheme=degree/paper_1995 (n=20000)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd /root/science-society\n",
    "\n",
    "START_MODE=\"per_node\"\n",
    "PER_NODE_SCHEME=\"degree\"\n",
    "\n",
    "if [ \"${START_MODE}\" = \"per_node\" ]; then\n",
    "  MODE_TAG=\"start=${START_MODE}__scheme=${PER_NODE_SCHEME}\"\n",
    "else\n",
    "  MODE_TAG=\"start=${START_MODE}\"\n",
    "fi\n",
    "\n",
    "CORPUS_DIR=\"./corpus/${MODE_TAG}\"\n",
    "DATASET_DIR=\"./datasets/${MODE_TAG}\"\n",
    "mkdir -p \"${DATASET_DIR}\"\n",
    "\n",
    "python scripts/make_hf_dataset.py \\\n",
    "  --corpus_root \"${CORPUS_DIR}\" \\\n",
    "  --out_dir \"${DATASET_DIR}\" \\\n",
    "  --years 1990 1991 1992 1993 1994 1995 \\\n",
    "  --domains news paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f16dce-886d-4f14-8b30-4c4412749919",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcd7665-f835-445e-b3cb-475ceca01b0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[vocab] loaded node tokens: 22577\n",
      "[tokenizer] added tokens: 22577 (new vocab size=50345)\n",
      "[init] initialized added node token embeddings: 19823, fallback_to_UNK: 0\n",
      "[mask] node_token_ids in tokenizer: 22577\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd /root/science-society\n",
    "\n",
    "START_MODE=\"per_node\"\n",
    "PER_NODE_SCHEME=\"degree\"\n",
    "if [ \"${START_MODE}\" = \"per_node\" ]; then\n",
    "  MODE_TAG=\"start=${START_MODE}__scheme=${PER_NODE_SCHEME}\"\n",
    "else\n",
    "  MODE_TAG=\"start=${START_MODE}\"\n",
    "fi\n",
    "\n",
    "y=1990\n",
    "DATASET_DIR=\"./datasets/${MODE_TAG}\"\n",
    "CKPT_DIR=\"./checkpoints/${MODE_TAG}\"\n",
    "mkdir -p \"${CKPT_DIR}\"\n",
    "\n",
    "python scripts/train_mlm_preinit.py \\\n",
    "  --dataset_dir \"${DATASET_DIR}/news_${y}\" \\\n",
    "  --node_vocab_txt ./tokenizer/concept_vocab_encoded.txt \\\n",
    "  --output_dir \"${CKPT_DIR}/news_${y}_seed\" \\\n",
    "  --num_train_epochs 3 \\\n",
    "  --learning_rate 5e-5 \\\n",
    "  --mlm_prob 0.15 \\\n",
    "  --mask_replace_prob 0.8 \\\n",
    "  --random_replace_prob 0.1\n",
    "\n",
    "python scripts/train_mlm_preinit.py \\\n",
    "  --dataset_dir \"${DATASET_DIR}/paper_${y}\" \\\n",
    "  --node_vocab_txt ./tokenizer/concept_vocab_encoded.txt \\\n",
    "  --output_dir \"${CKPT_DIR}/paper_${y}_seed\" \\\n",
    "  --num_train_epochs 3 \\\n",
    "  --learning_rate 5e-5 \\\n",
    "  --mlm_prob 0.15 \\\n",
    "  --mask_replace_prob 0.8 \\\n",
    "  --random_replace_prob 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191fd6d7-3d45-4c5c-b92d-8d91a095855c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /root/science-society\n",
    "\n",
    "START_MODE=\"per_node\"\n",
    "PER_NODE_SCHEME=\"degree\"\n",
    "if [ \"${START_MODE}\" = \"per_node\" ]; then\n",
    "  MODE_TAG=\"start=${START_MODE}__scheme=${PER_NODE_SCHEME}\"\n",
    "else\n",
    "  MODE_TAG=\"start=${START_MODE}\"\n",
    "fi\n",
    "\n",
    "init_y=1990\n",
    "DATASET_DIR=\"./datasets/${MODE_TAG}\"\n",
    "CKPT_DIR=\"./checkpoints/${MODE_TAG}\"\n",
    "mkdir -p \"${CKPT_DIR}\"\n",
    "\n",
    "d=\"news\"\n",
    "python scripts/train_annual_2stage_preinit.py \\\n",
    "  --init_dir \"${CKPT_DIR}/${d}_${init_y}_seed\" \\\n",
    "  --node_vocab_txt ./tokenizer/concept_vocab_encoded.txt \\\n",
    "  --years 1991 1992 1993 1994 \\\n",
    "  --dataset_template \"${DATASET_DIR}/${d}_{year}\" \\\n",
    "  --output_base_dir \"${CKPT_DIR}/${d}_annual\" \\\n",
    "  --exact_k_masks 2 \\\n",
    "  --num_train_epochs 3 \\\n",
    "  --learning_rate 3e-5 \\\n",
    "  --mask_replace_prob 0.8 \\\n",
    "  --random_replace_prob 0.1\n",
    "\n",
    "d=\"paper\"\n",
    "python scripts/train_annual_2stage_preinit.py \\\n",
    "  --init_dir \"${CKPT_DIR}/${d}_${init_y}_seed\" \\\n",
    "  --node_vocab_txt ./tokenizer/concept_vocab_encoded.txt \\\n",
    "  --years 1991 1992 1993 1994 \\\n",
    "  --dataset_template \"${DATASET_DIR}/${d}_{year}\" \\\n",
    "  --output_base_dir \"${CKPT_DIR}/${d}_annual\" \\\n",
    "  --exact_k_masks 2 \\\n",
    "  --num_train_epochs 3 \\\n",
    "  --learning_rate 3e-5 \\\n",
    "  --mask_replace_prob 0.8 \\\n",
    "  --random_replace_prob 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce59483-7446-4714-a66c-1c201bd82dd8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd /root/science-society\n",
    "\n",
    "START_MODE=\"per_node\"\n",
    "PER_NODE_SCHEME=\"degree\"\n",
    "if [ \"${START_MODE}\" = \"per_node\" ]; then\n",
    "  MODE_TAG=\"start=${START_MODE}__scheme=${PER_NODE_SCHEME}\"\n",
    "else\n",
    "  MODE_TAG=\"start=${START_MODE}\"\n",
    "fi\n",
    "\n",
    "CORPUS_DIR=\"./corpus/${MODE_TAG}\"\n",
    "CKPT_DIR=\"./checkpoints/${MODE_TAG}\"\n",
    "OUT_DIR=\"./outputs/${MODE_TAG}\"\n",
    "mkdir -p \"${OUT_DIR}\"\n",
    "\n",
    "python scripts/predict_neighbors_by_distance.py \\\n",
    "  --years 1991 \\\n",
    "  --domains news paper \\\n",
    "  --ckpt_root \"${CKPT_DIR}\" \\\n",
    "  --corpus_root \"${CORPUS_DIR}\" \\\n",
    "  --node_vocab_txt ./tokenizer/concept_vocab_encoded.txt \\\n",
    "  --targets automation productivity health_care legislation \\\n",
    "  --distance 1 \\\n",
    "  --topk 30 \\\n",
    "  --max_contexts 1000 \\\n",
    "  --out_root \"${OUT_DIR}/mask_pred\" \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9fc71f-db7c-4736-abee-7f9037d05f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "\n",
    "targets = [\n",
    "    \"automation\", \"productivity\", \"health_care\", \"legislation\",\n",
    "]\n",
    "\n",
    "base = \"/root/science-society/outputs/start=per_node__scheme=degree/mask_pred/dist=1/year=1991\"\n",
    "for t in targets:\n",
    "    path = f\"{base}/target={t}.tsv\"\n",
    "    print(f\"\\n============================== TARGET: {t} ==============================\")\n",
    "    df = pd.read_csv(path, sep=\"\\t\")\n",
    "    display(df.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d18f07d-9833-4812-bf70-63ed483ddd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "\n",
    "targets = [\n",
    "    \"automation\", \"productivity\", \"health_care\", \"legislation\",\n",
    "]\n",
    "\n",
    "base = \"/root/science-society/outputs/start=per_node__scheme=degree/mask_pred/dist=1/year=1994\"\n",
    "for t in targets:\n",
    "    path = f\"{base}/target={t}.tsv\"\n",
    "    print(f\"\\n============================== TARGET: {t} ==============================\")\n",
    "    df = pd.read_csv(path, sep=\"\\t\")\n",
    "    display(df.head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce2f577-33b2-46a9-b72a-734486c8ce91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
